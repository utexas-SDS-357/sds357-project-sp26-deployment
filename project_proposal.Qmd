---
title: "Project Proposal Testing"
author: "Asmit Chakraborty"
output:
    pdf_document: default
fontsize: 11pt
---

```{r}
beats <- read.csv("tx_houston_shapefiles/Houston_Police_Beats.csv", header=TRUE)
districts <- read.csv("tx_houston_shapefiles/Houston_Police_Districts.csv")
divisions <- read.csv("tx_houston_shapefiles/Houston_Police_Division.csv")
df <- read.csv("tx_houston_2023_01_26.csv")
```

```{r}
summary(df)

table(df$subject_race)
table(df$subject_sex)
table(df$type) # All vehicular
table(df$outcome) # All citation
table(df$district) # Could be interesting
table(df$citation_issued) # All TRUE
summary(df$speed) # about 30% of obs have speed tracked, what is speed vs posted speed?
```

Houston seems kind of boring. All stops received a cittion?

Potentially interesting problems
- clustering "high police stop risk" areas?
- Anything on differences between speeds in houston vs austin?

Newer (but have not looked closely at data yet for these)
- Clearly there are racial disparities in police stops. Are these more influenced
by who is stopped (age, race, sex, etc) or where the stop happened (district, etc)
  - Fit some sort of mixed effects model?

- Do stops/searches change based on day / night?
  - Idea is that, at night, police have a harder time profiling people

- For similar violations, are any demographic characteristics associated with a higher
likelihood of getting a citation or going to jail?

```{r}
df2 <- read.csv("~/Downloads/California.csv")
View(df2)
```


FIPS is the tract code

E_HU is the like estimated # of houses in that tract

so theres like house-level data which should lead us to be able to combine
tracts inside each of the like precincts or whatever those policing zones were
in that map i sent earlier

I think

Then we can take an average across all the tracts in each policing zone 

and if the SOPP data row {i} sits in precinct {j}, then the SVI column for row {i}
should be like the average of all the tract SVIs in precinct {j}

also like thats my ideal guess of how this would work out without having done any of it yet


```{r}
# ============================================================
# Add tract-level SVI (RPL_THEMES) to SOPP by SD policing region
# - Unweighted mean of RPL_THEMES across tracts intersecting each region
# - macOS-friendly: downloads TIGER zip locally, then reads shapefile
# - Includes progress prints
# ============================================================

library(sf)
library(dplyr)
library(readr)
library(httr)
library(jsonlite)
library(stringr)

sopp_path <- "~/Downloads/ca_san_diego_2020_04_01.csv"
svi_path  <- "~/Downloads/California.csv"
out_path  <- "./ca_san_diego_2020_04_01_with_svi.csv"

msg <- function(...) cat(sprintf("[%s] ", format(Sys.time(), "%H:%M:%S")), sprintf(...), "\n")

# ---- 1) Read SOPP ----
msg("Step 1/8: Reading SOPP from %s", sopp_path)
sopp <- read_csv(sopp_path, show_col_types = FALSE) %>%
  mutate(service_area_num = suppressWarnings(as.numeric(service_area)))
msg("  SOPP rows: %d | unique service_area (non-NA): %d",
    nrow(sopp), n_distinct(sopp$service_area[!is.na(sopp$service_area)]))
msg("  service_area_num non-NA: %d", sum(!is.na(sopp$service_area_num)))

# ---- 2) Read SVI and filter San Diego County ----
msg("Step 2/8: Reading SVI from %s", svi_path)
svi <- read_csv(svi_path, show_col_types = FALSE)

msg("  Filtering SVI to San Diego County (STCNTY == 6073) and keeping GEOID + RPL_THEMES")
svi_sd <- svi %>%
  filter(STCNTY == 6073) %>%
  transmute(
    GEOID = str_pad(as.character(FIPS), width = 11, side = "left", pad = "0"),
    RPL_THEMES = as.numeric(RPL_THEMES)
  ) %>%
  mutate(RPL_THEMES = ifelse(RPL_THEMES <= -999, NA_real_, RPL_THEMES)) %>%
  filter(!is.na(RPL_THEMES))

msg("  SVI SD tracts with non-missing RPL_THEMES: %d", nrow(svi_sd))

# ---- 3) Download + read 2010 Census tract polygons for San Diego County ----
tracts_url <- "https://www2.census.gov/geo/tiger/TIGER2010/TRACT/2010/tl_2010_06073_tract10.zip"
msg("Step 3/8: Downloading 2010 tract polygons (San Diego County) from TIGER")
msg("  URL: %s", tracts_url)

tmp_zip <- tempfile(fileext = ".zip")
tmp_dir <- file.path(tempdir(), paste0("tiger_", as.integer(Sys.time())))
dir.create(tmp_dir, showWarnings = FALSE, recursive = TRUE)

httr::GET(
  tracts_url,
  httr::write_disk(tmp_zip, overwrite = TRUE),
  httr::timeout(180)
) |>
  httr::stop_for_status()

msg("  Downloaded zip to: %s", tmp_zip)
msg("  Unzipping to: %s", tmp_dir)
unzip(tmp_zip, exdir = tmp_dir)

shp <- list.files(tmp_dir, pattern = "\\.shp$", full.names = TRUE)
stopifnot(length(shp) >= 1)
msg("  Reading shapefile: %s", shp[1])

tracts_sd <- sf::st_read(shp[1], quiet = TRUE) %>%
  mutate(GEOID = GEOID10) %>%
  left_join(svi_sd, by = "GEOID") %>%
  filter(!is.na(RPL_THEMES))

msg("  Tract polygons read: %d", nrow(tracts_sd))
msg("  CRS of tracts: %s", st_crs(tracts_sd)$input)

fetch_arcgis_layer_sf <- function(layer_url, where = "1=1", out_fields = "*", chunk_size = 300) {
  msg("  Querying feature count...")
  count_res <- httr::GET(
    paste0(layer_url, "/query"),
    query = list(where = where, returnCountOnly = "true", f = "json"),
    httr::timeout(60)
  )
  httr::stop_for_status(count_res)
  count <- httr::content(count_res, as = "parsed", type = "application/json")$count
  msg("  Feature count reported by server: %d", count)

  feats <- list()
  offset <- 0
  page <- 1

  while (offset < count) {
    msg("  Downloading page %d (offset=%d, n=%d)...", page, offset, chunk_size)

    res <- httr::GET(
      paste0(layer_url, "/query"),
      query = list(
        where = where,
        outFields = out_fields,
        returnGeometry = "true",
        f = "geojson",
        resultOffset = offset,
        resultRecordCount = chunk_size,
        outSR = 4326
      ),
      httr::timeout(120)
    )
    httr::stop_for_status(res)

    txt <- httr::content(res, as = "text", encoding = "UTF-8")
    gj <- jsonlite::fromJSON(txt, simplifyVector = FALSE)

    if (is.null(gj$features) || length(gj$features) == 0) {
      msg("  Server returned 0 features at offset=%d; stopping.", offset)
      break
    }

    feats <- c(feats, gj$features)
    offset <- offset + chunk_size
    page <- page + 1
  }

  msg("  Total features downloaded: %d. Converting to sf...", length(feats))
  fc <- list(type = "FeatureCollection", features = feats)
  sf::st_read(jsonlite::toJSON(fc, auto_unbox = TRUE), quiet = TRUE)
}

# ---- 4) Download SD policing polygons (Lawbeats_public) ----
lawbeats_layer <- "https://gis-public.sandiegocounty.gov/arcgis/rest/services/SDSO/Lawbeats_public/MapServer/0"
msg("Step 4/8: Downloading policing polygons (Lawbeats_public) from ArcGIS REST")
msg("  Layer: %s", lawbeats_layer)

beats <- fetch_arcgis_layer_sf(lawbeats_layer, out_fields = "BEAT,NAME") %>%
  mutate(BEAT = suppressWarnings(as.numeric(BEAT))) %>%
  filter(!is.na(BEAT))

msg("  Beat polygons downloaded: %d", nrow(beats))
msg("  Example BEAT codes: %s", paste(head(sort(unique(beats$BEAT))), collapse = ", "))
msg("  CRS of beats: %s", st_crs(beats)$input)

# ---- 5) Spatial join + compute mean RPL_THEMES per beat ----
msg("Step 5/8: Transforming to equal-area CRS (EPSG:3310) and spatially joining (intersects)")
ea_crs <- 3310

tracts_sd_ea <- st_transform(tracts_sd, ea_crs)
beats_ea     <- st_transform(beats, ea_crs)

msg("  Running st_join (this may take a bit)...")
joined <- st_join(
  tracts_sd_ea %>% select(GEOID, RPL_THEMES),
  beats_ea %>% select(BEAT, NAME),
  join = st_intersects,
  left = FALSE
)

msg("  Joined rows (tract-beat intersections): %d", nrow(joined))

msg("Step 6/8: Computing UNWEIGHTED mean RPL_THEMES per BEAT (unique tracts)")
beat_svi <- joined %>%
  st_drop_geometry() %>%
  distinct(BEAT, GEOID, .keep_all = TRUE) %>%
  group_by(BEAT) %>%
  summarise(
    n_tracts = dplyr::n(),
    SVI = mean(RPL_THEMES, na.rm = TRUE),
    .groups = "drop"
  )

msg("  Beat-level SVI rows: %d", nrow(beat_svi))
msg("  Summary of n_tracts per BEAT: min=%d, median=%d, max=%d",
    min(beat_svi$n_tracts), median(beat_svi$n_tracts), max(beat_svi$n_tracts))

# ---- 7) Merge SVI back onto SOPP ----
msg("Step 7/8: Merging beat-level SVI onto SOPP by service_area_num")
sopp2 <- sopp %>%
  left_join(beat_svi %>% select(BEAT, SVI), by = c("service_area_num" = "BEAT"))

msg("  SOPP rows after merge: %d", nrow(sopp2))
msg("  Share of stops with SVI assigned: %.3f", mean(!is.na(sopp2$SVI)))

# ---- 8) Write output + diagnostics ----
msg("Step 8/8: Writing output CSV to %s", out_path)
write_csv(sopp2, out_path)
msg("  Done.")

msg("Top missing service_area_num codes (present in SOPP but no SVI match):")
missing_codes <- sopp2 %>%
  filter(!is.na(service_area_num), is.na(SVI)) %>%
  count(service_area_num, sort = TRUE) %>%
  head(30)

print(missing_codes)
```
